{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97fe4a7",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Aqui eu só dou import em todas as partes importantes do código. Tenho que revisar pois acho que tem coisas que eu não uso de fato. Nota que os 3 ultimos imports são imports dos gráficos usados na resolução do problema usando a Adaline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3baa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35cd20",
   "metadata": {},
   "source": [
    "# Modelo CNN\n",
    "## O que cada coisa faz?\n",
    "Conv2d: Primeiro ele extrai os padrões uysando um kernel size de 3x3. Aplica numa área 3x3 e vai pegando os padrões da borda/cantos.\n",
    "\n",
    "AvgPool2d: reduz resolução (downsampling) pegando a média do 2x2.\n",
    "\n",
    "BatchNorm: Estabiliza o treino.\n",
    "\n",
    "Sigmoid: Função de ativação sigmoide. (Perguntar o que é a SILU e se era pra usar)\n",
    "\n",
    "Obs: Esse valor de 1600 no MLP pelo valor que o tensor fica no final. A lógica é que ele passa pelo Conv2d e acaba multiplicando os fatores por uma formula e ganha números a mais a cada dimensão. Aí ele passa pelo batch e fica menor. Fazendo com que a dimensão dos dois ultimos algarismos fique menos mas a primeira continua grande. Ou seja, usando a formula de tamanho espacial, isso fica (28-3+0)/1+1=26 ao passar no primeiro bloco, então o tensor tem tamanho de (32,26,26), passa pelo Batch e reduz as dimensões em dois sem ser a informação adquirida da Conv2d, ficando (32,13,13) e resultando no final em uma saída de 32 * 13 * 13 = 5408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782a1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeloMNIST(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.bloco1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), bias=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)), # Aqui eu to usando MaxPool que é pegar o maximo, nesse caso, de um quadrado 2d. Eu poderia usar o average(média do quadrado) ou o min\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "        )\n",
    "        self.bloco2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), bias=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)), # Aqui eu to usando MaxPool que é pegar o maximo, nesse caso, de um quadrado 2d. Eu poderia usar o average(média do quadrado) ou o min\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=1600, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=10),\n",
    "        )\n",
    "    def forward(self, dados: torch.Tensor) -> torch.Tensor:\n",
    "        saida = self.bloco1(dados)\n",
    "        saida = self.bloco2(saida)\n",
    "        saida = self.mlp(saida.flatten(1))\n",
    "        return saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69acc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreinandoMNIST:\n",
    "    def __init__(self):\n",
    "        self.tamanhoBatch = 32\n",
    "        self.embaralhar = True\n",
    "        #self.workers = 0\n",
    "        self.lr = 0.002\n",
    "        self.epocas = 4\n",
    "        self.dispositivo = torch.device(\"cpu\")\n",
    "        \n",
    "        self.historicoLoss = []\n",
    "        self.historicoAcc = []\n",
    "\n",
    "        self.inicializarDataset()\n",
    "        self.inicializarModelo()\n",
    "            \n",
    "    def inicializarDataset(self):\n",
    "        # transformando os valores em 0 e 1 e em tensores. ou seja, agora terei uma imagem (x, 0/1, 0/1) a cada pixel.\n",
    "        transf = Compose([ToTensor(), Normalize(mean=(0.1307,), std=(0.3081,))])\n",
    "        \n",
    "        dsTreino = MNIST(root=\"dataset\", train=True, download=True, transform=transf)\n",
    "        \n",
    "        self.loaderTreino = DataLoader(\n",
    "            dsTreino, \n",
    "            batch_size=self.tamanhoBatch, \n",
    "            shuffle=self.embaralhar, \n",
    "        )\n",
    "        \n",
    "        dsTeste = MNIST(root=\"dataset\", train=False, download=True, transform=transf)\n",
    "        self.loaderTeste = DataLoader(\n",
    "            dsTeste,\n",
    "            batch_size=self.tamanhoBatch,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def inicializarModelo(self):\n",
    "        self.modelo = ModeloMNIST()\n",
    "        self.modelo.to(self.dispositivo)\n",
    "        self.criterio = nn.CrossEntropyLoss()\n",
    "        #self.criterio = nn.MSELoss()\n",
    "\n",
    "        # parametro - lr * gradiente// Perguntar para Leonardo se era pra eu usar o otimizador ou fazer na mao a atualização dos pesos\n",
    "        \n",
    "        # self.otimizador = optim.AdamW(\n",
    "        #     self.modelo.parameters(), \n",
    "        #     lr=self.lr, \n",
    "        #     # weight decay entraria aqui mas por que eu botaria ele e como que ele funciona\n",
    "        # )\n",
    "\n",
    "    def treinar(self):\n",
    "        print(\"Treinando...\\n\\n\")\n",
    "        self.modelo.train()\n",
    "        \n",
    "        for epoca in range(self.epocas):\n",
    "            perdaCorrente = 0.0\n",
    "            for idx, (imagens, rotulos) in enumerate(self.loaderTreino):\n",
    "                imagens, rotulos = imagens.to(self.dispositivo), rotulos.to(self.dispositivo)\n",
    "                \n",
    "                #zerando gradiente, se eu usar optimizerAdam aqui é optimizer e usa o optimizer/inicializa ele la em cima\n",
    "                self.modelo.zero_grad()\n",
    "                \n",
    "                saidas = self.modelo(imagens)\n",
    "                \n",
    "                # one hot pro mse\n",
    "                #rotulosOneHot = F.one_hot(rotulos, num_classes=10).float()\n",
    "                \n",
    "                #se eu quiser testar dnv o crossEntropy, er so colocar rotulos auqi ao inves de rotulos one hot e rodar\n",
    "                perda = self.criterio(saidas, rotulos)\n",
    "                perda.backward()\n",
    "                \n",
    "                # fazendo a atualização manual\n",
    "                with torch.no_grad():\n",
    "                    for param in self.modelo.parameters():\n",
    "                            # w = w - learning_rate * gradiente\n",
    "                            param -= self.lr * param.grad\n",
    "\n",
    "                perdaCorrente += perda.item()\n",
    "                \n",
    "                # a cada 50 passos ele faz um log\n",
    "                if idx % 50 == 49:\n",
    "                    mediaPerda = perdaCorrente / 50\n",
    "                    \n",
    "                    _, predicoes = torch.max(saidas, 1)\n",
    "                    AcuraciaBatch = (predicoes == rotulos).float().mean().item()\n",
    "                    \n",
    "                    self.historicoLoss.append(mediaPerda)\n",
    "                    self.historicoAcc.append(AcuraciaBatch)\n",
    "\n",
    "\n",
    "                    print(f\"Época {epoca+1} | Passos: {idx + 1:<4} | Perda: {mediaPerda:.3f} | Acurácia Batch: {AcuraciaBatch:.3f}\")\n",
    "                    perdaCorrente = 0.0\n",
    "        \n",
    "\n",
    "    def avaliar(self):\n",
    "        print(\"Avaliação:\\n\\n\")\n",
    "        self.modelo.eval()\n",
    "        \n",
    "        reais = []\n",
    "        previstos = []\n",
    "        imagensTeste = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imagens, rotulos in self.loaderTeste:\n",
    "                imagensGpu = imagens.to(self.dispositivo)\n",
    "                saida = self.modelo(imagensGpu)\n",
    "                _, previsao = torch.max(saida.data, 1)\n",
    "\n",
    "                previstos.extend(previsao.cpu().numpy())\n",
    "                reais.extend(rotulos.numpy()) \n",
    "                imagensTeste.append(imagens.numpy()) \n",
    "\n",
    "        # numpy\n",
    "        self.XTest = np.concatenate(imagensTeste)\n",
    "        self.yReal = np.array(reais)\n",
    "        self.yPred = np.array(previstos)\n",
    "\n",
    "        acuracia = accuracy_score(self.yReal, self.yPred)\n",
    "        self.precision, self.recall, self.fscore, _ = precision_recall_fscore_support(self.yReal, self.yPred, average=\"macro\")\n",
    "        _, _, self.fscoreC, _ = precision_recall_fscore_support(self.yReal, self.yPred, average=None)\n",
    "        \n",
    "        print(f\"{acuracia=:.3f}\")\n",
    "        print(f\"{self.precision=:.3f}\")\n",
    "        print(f\"{self.recall=:.3f}\")\n",
    "        print(f\"{self.fscore=:.3f}\")\n",
    "\n",
    "        print(f\"Acurácia Final no Teste: {acuracia:.3f}\")\n",
    "\n",
    "    def plotarResultados(self):\n",
    "        print(\"--- Gerando Gráficos ---\")\n",
    "        plt.figure(figsize=(15, 12))\n",
    "\n",
    "        plt.subplot(3, 2, 1)\n",
    "        plt.plot(self.historicoLoss, label='Erro (MSE)', color='blue', linewidth=2)\n",
    "        plt.title(\"Redução do Erro (Treino)\")\n",
    "        plt.xlabel(\"Iterações (x50)\")\n",
    "        plt.ylabel(\"Erro\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 2, 2)\n",
    "        plt.plot(self.historicoAcc, label='Acurácia', color='orange', linewidth=2)\n",
    "        plt.title(\"Aumento da Acurácia (Treino)\")\n",
    "        plt.xlabel(\"Iterações (x50)\")\n",
    "        plt.ylabel(\"Taxa de Acerto\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 2, 3)\n",
    "        cm = confusion_matrix(self.yReal, self.yPred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.title(\"Matriz de Confusão (Teste)\")\n",
    "        plt.ylabel(\"Real\")\n",
    "        plt.xlabel(\"Previsto\")\n",
    "\n",
    "        plt.subplot(3, 2, 4)\n",
    "        classes = [str(i) for i in range(10)]\n",
    "        plt.bar(classes, self.fscoreC, color='purple', alpha=0.7)\n",
    "        plt.title(\"F1-Score por Dígito (Teste)\")\n",
    "        plt.xlabel(\"Dígito\")\n",
    "        plt.ylabel(\"F1-Score\")\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        for i, v in enumerate(self.fscoreC):\n",
    "            plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontsize=9)\n",
    "\n",
    "        rng = np.random.default_rng()\n",
    "        nSamples = min(len(self.XTest), 6)\n",
    "        indicesTeste = rng.choice(len(self.XTest), nSamples, replace=False)\n",
    "\n",
    "        for i, idx in enumerate(indicesTeste):\n",
    "            plt.subplot(3, 6, 13 + i)\n",
    "            img = self.XTest[idx].reshape(28, 28)\n",
    "            pred = self.yPred[idx]\n",
    "            real = self.yReal[idx]\n",
    "            \n",
    "            plt.imshow(img, cmap='gray')\n",
    "            cor = 'green' if pred == real else 'red'\n",
    "            plt.title(f\"P: {pred} / R: {real}\", color=cor, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08925e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando...\n",
      "\n",
      "\n",
      "Época 1 | Passos: 50   | Perda: 2.189 | Acurácia Batch: 0.469\n",
      "Época 1 | Passos: 100  | Perda: 1.853 | Acurácia Batch: 0.500\n",
      "Época 1 | Passos: 150  | Perda: 1.492 | Acurácia Batch: 0.781\n",
      "Época 1 | Passos: 200  | Perda: 1.158 | Acurácia Batch: 0.844\n",
      "Época 1 | Passos: 250  | Perda: 0.953 | Acurácia Batch: 0.844\n",
      "Época 1 | Passos: 300  | Perda: 0.757 | Acurácia Batch: 0.875\n",
      "Época 1 | Passos: 350  | Perda: 0.668 | Acurácia Batch: 0.938\n",
      "Época 1 | Passos: 400  | Perda: 0.567 | Acurácia Batch: 0.875\n",
      "Época 1 | Passos: 450  | Perda: 0.490 | Acurácia Batch: 0.906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     CNN = TreinandoMNIST()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mCNN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtreinar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     CNN.avaliar()\n\u001b[32m      5\u001b[39m     CNN.plotarResultados()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mTreinandoMNIST.treinar\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# one hot pro mse\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m#rotulosOneHot = F.one_hot(rotulos, num_classes=10).float()\u001b[39;00m\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m#se eu quiser testar dnv o crossEntropy, er so colocar rotulos auqi ao inves de rotulos one hot e rodar\u001b[39;00m\n\u001b[32m     67\u001b[39m perda = \u001b[38;5;28mself\u001b[39m.criterio(saidas, rotulos)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mperda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# fazendo a atualização manual\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuário\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuário\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuário\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CNN = TreinandoMNIST()\n",
    "    CNN.treinar()\n",
    "    CNN.avaliar()\n",
    "    CNN.plotarResultados()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa7cfd",
   "metadata": {},
   "source": [
    "# -Utilizando Sigmoide---------------------------------\n",
    "### •Com MSE e ONE HOT;\n",
    "&emsp; Usando Adam Optimizer o resultado varia de 94% até 99%. \n",
    "&emsp; Usando atualização manual dos pesos com 32 neurônios: Min de 78%(5) e Max de 92%(1). Utilizando 128 neurônios: Min 83% e Max de 94%. Média de 89,3%\n",
    "### •Com CROSS-ENTROPY/Entropia cruzada \n",
    "&emsp; Usando atualização manual dos pesos com 32 neurônios: 94%(4) até 99%(2). Média de 94,9%. Utilizando 128 neurônios: Média de 95% ou seja, sem mudança subtancial.\n",
    "\n",
    "# -Utilizando ReLU/Rectifiec linear unit---------------\n",
    "### •Com MSE e ONE HOT\n",
    "&emsp; Usando atualização manual dos pesos com 32 neurônios: Média de 94,9%. \n",
    "### •Com CROSS-ENTROPY/Entropia cruzada\n",
    "&emsp; Usando atualização manual dos pesos com 32 neurônios: Média de 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3540733",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
